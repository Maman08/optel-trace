ğŸ¬ The Story of Building an End-to-End Monitoring System for a Django + AWS App

Once upon a time, in a dev world not so far away, you (the backend developer) were building a Django-based application. Your backend was running on localhost:8000 for now, but it was growing quicklyâ€”and soon it would be in production.

As your app evolved, it started using AWS services like:

    Amazon RDS for storing SQL data

    Amazon S3 for uploading and accessing media files

    Maybe even Lambda functions or SNS for notifications

But there was a growing fear...

ğŸ˜¨ "What if something breaks and I donâ€™t even know?"

    What if a request to RDS takes too long?

    What if S3 uploads silently fail?

    What if your API becomes unavailable and your lead knows before you do?

You needed eyes on everything.

So, you decided to create an enterprise-grade monitoring system. Not just logs. Not just dashboards. The whole thing â€” metrics, logs, traces, and uptime monitoring. Like a full-blown control room.

Letâ€™s break it down scene-by-sceneâ€¦

ğŸ¯ Scene 1: Metrics â€” The Pulse of Your App

Metrics tell you things like:

    How many requests hit your Django app?

    Whatâ€™s the error rate?

    Whatâ€™s the memory usage of your server?

    How many times did a DB query timeout?

You decided to use Amazon Managed Prometheus (AMP) â€” because:

ğŸŸ¢ Why AMP?

    Itâ€™s Prometheus without managing the servers.

    You just push metrics to it, and it stores them in a time-series format.

    You can query it using PromQL, a powerful query language.

    It can scrape your Django metrics and API uptime data.

But to collect those metrics, you had to expose them from Django.

ğŸ’¡ How? You installed django-prometheus.

You added this in settings.py:

INSTALLED_APPS += ['django_prometheus']

And in your urls.py:

from django_prometheus import exports urlpatterns += [path('metrics/', exports.ExportToDjangoView.as_view())]

Now when you open localhost:8000/metrics, Prometheus (or AMP) can scrape all these metrics:

    django_http_requests_total

    django_http_request_duration_seconds

    django_db_execute_total

You deployed an OpenTelemetry Collector locally to pull those metrics and push them to AMP.

âœ… Done: You now had app-level metrics flowing into Amazon Managed Prometheus.

But waitâ€¦

ğŸ˜ "What about AWS RDS metrics?"

You canâ€™t scrape AWS like you scrape Django.

Enter...

â˜ï¸ Scene 2: AWS Metrics â€” Eyes Inside the Cloud

AWS services like RDS, S3, Lambda emit their own metrics â€” but only to one place: CloudWatch.

ğŸŸ¢ Why CloudWatch?

    AWS automatically sends metrics there.

    For RDS, you get CPU utilization, DB connections, slow query counts.

    For S3, you get metrics like 4xx error rates, object size, etc.

â›” If you donâ€™t use CloudWatch, you canâ€™t access these AWS-level metrics.

ğŸ’¡ Solution: You configured your Amazon Managed Grafana to read from both AMP and CloudWatch.

This gave you a unified dashboard: Django + AWS metrics side-by-side.

âœ… Done: You now had complete metrics.

ğŸ¬ Scene 3: Logs â€” What Actually Happened?

Metrics give you summaries, but logs give you raw details.

Your Django app was printing logs â€” using Pythonâ€™s logging module.

So, you decided to forward these logs to CloudWatch Logs.

ğŸŸ¢ Why CloudWatch Logs?

    Native integration with AWS.

    One place for Django logs, Lambda logs, and RDS logs (like slow query logs).

    Supports log search, filtering, retention.

ğŸ’¡ You installed and configured awslogs agent or used OpenTelemetry Collector to push Django logs to CloudWatch.

ğŸ¯ Bonus: RDS logs like â€œslow queriesâ€ or â€œconnection errorsâ€ are automatically sent to CloudWatch if you enable them in DB settings.

âœ… Done: Now, logs from both your backend and AWS are searchable in one place.

ğŸ•¸ Scene 4: Tracing â€” Following the Journey of a Request

Imagine someone hits this API:

http://beta.janitri.in/api/daily-report/?password=zQXA1dz5Yc

That request might:

    Hit Django

    Fetch data from RDS

    Upload or read a file from S3

ğŸŸ  Problem: If the response is slow, how do you know where the slowness came from?

ğŸŸ¢ Solution: Use OpenTelemetry in Django to trace each request.

OpenTelemetry creates a trace for every incoming HTTP request and every outgoing database or HTTP call. You instrument Django and its database layer.

You install opentelemetry-sdk and Django instrumentation:

pip install opentelemetry-distro opentelemetry-sdk opentelemetry-instrumentation-django

Then run Django with tracing:

opentelemetry-instrument python manage.py runserver

You configure OpenTelemetry Collector to forward these traces to AWS X-Ray.

ğŸŸ¢ Why X-Ray?

    AWS native

    Visualizes full request journey

    Integrates with S3, RDS, Lambda â€” so you can see traces across AWS too

âœ… Done: You can now trace every request from user â†’ Django â†’ AWS services.

ğŸ” Scene 5: Uptime Monitoring

You were told this endpoint is critical:

http://beta.janitri.in/api/daily-report/?password=zQXA1dz5Yc

You donâ€™t want to know itâ€™s down because your lead called you.

ğŸ’¡ Solution: Prometheus Blackbox Exporter

    It pings HTTP endpoints at intervals.

    You configure it to check status codes, response time.

    Prometheus scrapes it and shows in Grafana.

You added this to your blackbox config:

modules: http_2xx: prober: http timeout: 5s http: valid_http_versions: ["HTTP/1.1", "HTTP/2"] method: GET

And this to prometheus.yml:

    job_name: 'api-monitor' metrics_path: /probe params: module: [http_2xx] static_configs:

        targets:

            http://beta.janitri.in/api/daily-report/?password=zQXA1dz5Yc relabel_configs:

        source_labels: [address] target_label: __param_target

        source_labels: [__param_target] target_label: instance

        target_label: address replacement: blackbox-exporter:9115

âœ… Done: Now your dashboard lights up when the API is down.

ğŸ§  Final Chapter: Grafana â€” The Single Pane of Glass

Now that metrics, logs, and traces are flowing in from:

    Django (via OTEL)

    RDS and S3 (via CloudWatch)

    Traces (via X-Ray)

    API Uptime (via Blackbox)

You connected everything to Amazon Managed Grafana.

ğŸŸ¢ Why?

    Because you donâ€™t want to open 5 consoles.

    Grafana lets you show metrics from AMP, CloudWatch, and traces from X-Ray â€” all in one dashboard.

ğŸ‰ The End (or Beginning)

You now have:

    âœ… Metrics from backend and AWS

    âœ… Logs from Django, RDS, etc.

    âœ… Distributed tracing

    âœ… API uptime alerts

    âœ… One dashboard for all of it

And the best part? You avoided service redundancy.

Instead of adding Loki (another log system), you used CloudWatch (already integrated). Instead of self-hosting Grafana, you used AWS-managed Grafana.

Now, whenever something fails â€” youâ€™ll be the first to know, and the first to fix.

Want a downloadable version of this story in PDF or diagram form? I can generate that for you too.
You said:
this is great great

one thing i would like to ask 
do we think we should also discuss on do we need to store this data or not?
also what are frequency 
ChatGPT said:

So glad you loved it! ğŸ‰ That's an excellent next question â€” and yes, it's very important to discuss:

    â“ Should we store this observability data?

    â±ï¸ What frequency should we collect it at?

Letâ€™s go deeper into both â€” because these two factors directly affect cost, performance, and how useful your monitoring actually is.

ğŸ¯ Part 1: Should You Store Metrics, Logs, Traces?

Letâ€™s take it by data type:

    ğŸ“Š Metrics (CPU, memory, requests per second, etc.)

âœ… Yes, store them.

Why?

    You want to see historical patterns (e.g., was traffic higher last week?).

    Helps you correlate issues â€” â€œohh, memory started increasing 2 hours before that crash.â€

    Required for alerting on trends (e.g., CPU > 80% for 5 minutes).

How Long?

    Short-term: 15 days for fast-moving dashboards

    Mid-term: 60 days for investigations

    Long-term: 1 year for audit/compliance or capacity planning

AMP supports retention policies. Store detailed metrics (per-second) for a few days, aggregate ones (avg/5min) longer.

    ğŸªµ Logs (Django logs, RDS slow queries, etc.)

âœ… Yes, store logs â€” but be smart about it.

Why?

    Logs are often your "black box flight recorder."

    They help you debug why something failed.

    Essential for error investigations.

But logs can get expensive (especially noisy Django logs), so:

    Use filters to drop unimportant logs (like access logs with 200 OK)

    Set different retention policies: e.g., keep errors for 90 days, info logs for 7 days

    Enable compression/archiving after 30 days

CloudWatch lets you set retention per log group. You can even archive to S3 (cheap!) if needed.

    ğŸ•¸ Traces (OpenTelemetry to X-Ray)

ğŸ”„ Store recent traces â€” not all.

Why?

    Traces are extremely helpful for debugging performance issues.

    But they are heavier and can grow quickly in volume.

What you do:

    Sample traces (e.g., only 10% or 1% of requests get traced unless thereâ€™s an error).

    Store detailed traces for 7-30 days.

    Only keep error traces long-term.

OpenTelemetry supports sampling strategies like probabilistic sampling or tail-based sampling. Use it to reduce cost.

â€”

ğŸ§  TL;DR on Storage:
Data Type	Store?	Retention (Typical)	Notes
Metrics	âœ… Yes	15d - 1y	Store aggregates longer, raw shorter
Logs	âœ… Yes	7d - 90d+	Filter out noise, keep errors longer
Traces	âœ… Yes (sampled)	7d - 30d	Keep only errors or performance outliers long-term

â±ï¸ Part 2: What Frequency Should We Collect?

Frequency impacts how real-time your dashboard is, and how expensive it gets.

    Metrics:

    Typical: every 15 seconds to 1 minute

    For CPU/memory/network â†’ 1 minute is fine

    For API request metrics â†’ 15s or 30s gives better alerting

Prometheus scrapes by interval â€” and AWS CloudWatch defaults are 1-minute granularity. For finer metrics, you can enable â€œdetailed monitoringâ€ (every 1 second), but it costs more.

    Logs:

    Real-time ingestion is standard â€” as logs are emitted, they are pushed to CloudWatch.

    You donâ€™t usually "sample" logs, but you can filter out noisy ones before shipping.

    Traces:

    Collected per request, but sampling is the key

    You donâ€™t want to trace every request â€” use:

        100% sampling for dev

        5â€“10% sampling for production

        100% for error responses

â€”

ğŸ”¥ Bonus Tip: Alerting Frequency

You want your alerting pipeline to evaluate every 30sâ€“1m ideally. This gives near real-time alerts without flapping.

â€”

ğŸ“¦ Storage Cost Optimization Tips

    Use retention policies â€” never store â€œforeverâ€

    Aggregate metrics after 7 days (from 15s â†’ 5min resolution)

    Archive logs older than 30 days to S3

    Use X-Ray trace sampling aggressively

    Alert only on aggregated data (e.g., 5xx > 5 in 1m)

ğŸ”š Final Thought

Yes â€” storing the observability data is crucial. But smart storage (tiered, filtered, aggregated) ensures you get the visibility you want without ballooning costs or overwhelming dashboards.

Want me to write this up as a â€œMonitoring Storage Strategyâ€ document or turn it into a diagram like the previous one?
You said:
some  more questions
amp stores only metrics for some period ~15days if i'm not wrong,right?it doent store logs,traces
2nd q. if we are filtering the log than where it needs to be done in backend code or where at what level
3rd. CloudWatch lets you set retention per log group.(you said),,,,is it default behaviour of cloud watch?also what that mean 
4th . You can even archive to S3 (cheap!),what is it??
ChatGPT said:

Absolutely, letâ€™s go through your questions one by one with crystal clarity and examples where needed. Youâ€™re thinking in the right direction â€” these are the kind of questions good infra engineers and backend leads ask. ğŸ‘‡
1ï¸âƒ£ Does Amazon Managed Prometheus (AMP) store only metrics?

âœ… Yes, 100% correct.

    AMP is for metrics only. It is a Prometheus-compatible time-series database.

    It does not store logs or traces.

    Metrics are typically stored with a retention period of:

        15 days for high-resolution metrics

        You can set longer if you use recording rules to aggregate (e.g., 5m averages)

ğŸ“¦ Example:

You are collecting Django metrics like http_request_duration_seconds. Prometheus (AMP) stores these as time-series: timestamped float numbers.

ğŸŸ¨ It wonâ€™t store:

    Logs like: POST /api/register - 200 OK - 42ms

    Traces like: request came â†’ DB took 200ms â†’ S3 took 100ms

Those go elsewhere (CloudWatch for logs, X-Ray for traces).
2ï¸âƒ£ Where do we filter logs before shipping?

Great question â€” and this is important for cost control and noise reduction.

ğŸ§© Where to filter logs:

You can filter logs at different levels depending on what tools you're using:
Layer	Tool	How You Filter
Django code	logging module	Use different log levels (INFO, DEBUG, ERROR), handlers, or filters to suppress noisy logs
OTEL Collector	processors and exporters	Filter logs based on attributes (e.g., drop all INFO logs)
CloudWatch Agent	Log group configs	Choose which log files to ship or not
Lambda	Use Lambda environment to send only specific logs	

ğŸ’¡ Best Practice:

    Filter noisy log levels (like DEBUG, INFO) at the code or agent level.

    Filter noisy endpoints (e.g., /healthz, /favicon.ico) if not helpful.

    You can use the OpenTelemetry Collector to drop/filter specific logs too before forwarding to CloudWatch.

3ï¸âƒ£ â€œCloudWatch lets you set retention per log group.â€ What does this mean?

CloudWatch organizes logs into buckets called log groups.

ğŸ‘‹ Example:

You may have log groups like:

    /aws/lambda/my-function

    /ecs/my-backend-service

    /django-app/production

ğŸ” Retention is per log group:

You can configure how long logs in each group are retained â€” 1 day, 7 days, 30 days, 1 year, etc.

âœ… Default behavior:

    By default, log groups have infinite retention (yes, youâ€™ll keep paying unless you set retention!).

Thatâ€™s why itâ€™s critical to go into CloudWatch and set retention manually via AWS Console or Terraform/CDK.

ğŸ‘¨â€ğŸ”§ Example via CLI:

aws logs put-retention-policy \
  --log-group-name "/django-app/production" \
  --retention-in-days 14

â³ Tip: Set short retention for access logs, longer for error logs.
4ï¸âƒ£ â€œYou can archive logs to S3â€ â€” What does that mean?

CloudWatch lets you export older logs to Amazon S3. This is useful when:

    You donâ€™t want to keep paying for logs in CloudWatch (it's expensive!)

    But you want to retain them long-term (e.g., audit logs, error history)

ğŸ§  What happens:

    Logs older than X days are moved to S3 bucket in JSON or GZ format

    It becomes "cold storage" â€” cheap and durable

    You donâ€™t use it in real-time dashboards, but you can go back and search if needed

ğŸ›  How to do it:

    You set up an export task in CloudWatch or automate it via Lambda/Scheduler

    Choose the log group, date range, and target S3 bucket

    CloudWatch exports logs to S3 in batches

ğŸ”’ Example use-case:

    â€œKeep 7 days of logs in CloudWatch for real-time viewing, then archive weekly to S3 and retain for 1 year for compliance.â€

ğŸ”š Summary
Question	Answer
AMP stores only metrics?	âœ… Yes, no logs/traces. Only time-series metrics
Where to filter logs?	Preferably at app level or OTEL Collector before CloudWatch
Retention per log group?	âœ… Yes, but default is infinite â€” set it yourself
Archive to S3?	âœ… Export old logs to S3 for cheap, long-term storage
